{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install -q efficientnet\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import efficientnet.tfkeras as efn\n",
    "import dill\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('melanoma-384x384')\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "AUG_BATCH = BATCH_SIZE\n",
    "IMAGE_SIZE = [384, 384]\n",
    "# Seed\n",
    "SEED = 123\n",
    "# Learning rate\n",
    "LR = 0.0003\n",
    "# cutmix prob\n",
    "cutmix_rate = 0.30\n",
    "\n",
    "# training filenames directory\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n",
    "# test filenames directory\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')\n",
    "# submission file\n",
    "SUB = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 26500 training images, 6625 validation images, 10982 unlabeled test images\n"
     ]
    }
   ],
   "source": [
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
    "        \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
    "    \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "def transform(image, label):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    tmp = random.uniform(0, 1)\n",
    "    if 0 < tmp <= 0.1:\n",
    "        rot = 15.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.1 < tmp <= 0.2:\n",
    "        rot = 30.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.2 < tmp <= 0.3:\n",
    "        rot = 45.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.3 < tmp <= 0.4:\n",
    "        rot = 60.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.4 < tmp <= 0.5:\n",
    "        rot = 75.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.5 < tmp <= 0.6:\n",
    "        rot = 90.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.6 < tmp <= 0.7:\n",
    "        rot = 110.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.7 < tmp <= 0.8:\n",
    "        rot = 130.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.8 < tmp <= 0.9:\n",
    "        rot = 150.0 * tf.random.normal([1],dtype='float32')\n",
    "    elif 0.9 < tmp <= 1.0:\n",
    "        rot = 180.0 * tf.random.normal([1],dtype='float32')\n",
    "    shr = 5. * tf.random.normal([1],dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
    "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
    "    h_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "    w_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "  \n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = K.cast(idx2,dtype='int32')\n",
    "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    d = tf.gather_nd(image['inp1'],tf.transpose(idx3))\n",
    "        \n",
    "    return {'inp1': tf.reshape(d,[DIM,DIM,3]), 'inp2': image['inp2']}, label\n",
    "\n",
    "# function to apply cutmix augmentation\n",
    "def cutmix(image, label):\n",
    "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
    "    # output - a batch of images with cutmix applied\n",
    "    \n",
    "    DIM = IMAGE_SIZE[0]    \n",
    "    imgs = []; labs = []\n",
    "    \n",
    "    for j in range(BATCH_SIZE):\n",
    "        \n",
    "        #random_uniform( shape, minval=0, maxval=None)        \n",
    "        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n",
    "        P = tf.cast(tf.random.uniform([], 0, 1) <= cutmix_rate, tf.int32)\n",
    "        \n",
    "        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n",
    "        k = tf.cast(tf.random.uniform([], 0, BATCH_SIZE), tf.int32)\n",
    "        \n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n",
    "        y = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n",
    "        \n",
    "        # Beta(1, 1)\n",
    "        b = tf.random.uniform([], 0, 1) # this is beta dist with alpha=1.0\n",
    "        \n",
    "\n",
    "        WIDTH = tf.cast(DIM * tf.math.sqrt(1-b),tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
    "        \n",
    "        # MAKE CUTMIX IMAGE\n",
    "        one = image['inp1'][j,ya:yb,0:xa,:]\n",
    "        two = image['inp1'][k,ya:yb,xa:xb,:]\n",
    "        three = image['inp1'][j,ya:yb,xb:DIM,:]        \n",
    "        #ya:yb\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "\n",
    "        img = tf.concat([image['inp1'][j,0:ya,:,:],middle,image['inp1'][j,yb:DIM,:,:]],axis=0)\n",
    "        imgs.append(img)\n",
    "        \n",
    "        # MAKE CUTMIX LABEL\n",
    "        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n",
    "        lab1 = label[j,]\n",
    "        lab2 = label[k,]\n",
    "        labs.append((1-a)*lab1 + a*lab2)\n",
    "\n",
    "    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n",
    "    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE, 1))\n",
    "    return {'inp1': image2, 'inp2': image['inp2']}, label2\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# function to decode our images (normalize and reshape)\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    # convert image to floats in [0, 1] range\n",
    "    image = tf.cast(image, tf.float32) / 255.0 \n",
    "    # explicit size needed for TPU\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "# this function parse our images and also get the target variable\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        # tf.string means bytestring\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        # shape [] means single element\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        # meta features\n",
    "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        \n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['target'], tf.float32)\n",
    "    # meta features\n",
    "    data = {}\n",
    "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
    "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
    "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
    "    # returns a dataset of (image, label, data)\n",
    "    return image, label, data\n",
    "\n",
    "# this function parse our image and also get our image_name (id) to perform predictions\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        # tf.string means bytestring\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        # shape [] means single element\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        # meta features\n",
    "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    image_name = example['image_name']\n",
    "    # meta features\n",
    "    data = {}\n",
    "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
    "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
    "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
    "    # returns a dataset of (image, key, data)\n",
    "    return image, image_name, data\n",
    "    \n",
    "def load_dataset(filenames, labeled = True, ordered = False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
    "    \n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        # disable order, increase speed\n",
    "        ignore_order.experimental_deterministic = False \n",
    "        \n",
    "    # automatically interleaves reads from multiple files\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    # use data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) \n",
    "    return dataset\n",
    "\n",
    "# function for training and validation dataset\n",
    "def setup_input1(image, label, data):\n",
    "    \n",
    "    # get anatom site general challenge vectors\n",
    "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
    "    \n",
    "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
    "    \n",
    "    tabular = tf.stack(tab_data + anatom)\n",
    "    \n",
    "    return {'inp1': image, 'inp2':  tabular}, label\n",
    "\n",
    "# function for the test set\n",
    "def setup_input2(image, image_name, data):\n",
    "    \n",
    "    # get anatom site general challenge vectors\n",
    "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
    "    \n",
    "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
    "    \n",
    "    tabular = tf.stack(tab_data + anatom)\n",
    "    \n",
    "    return {'inp1': image, 'inp2':  tabular}, image_name\n",
    "\n",
    "# function for the validation (image name)\n",
    "def setup_input3(image, image_name, target, data):\n",
    "    \n",
    "    # get anatom site general challenge vectors\n",
    "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
    "    \n",
    "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
    "    \n",
    "    tabular = tf.stack(tab_data + anatom)\n",
    "    \n",
    "    return {'inp1': image, 'inp2':  tabular}, image_name, target\n",
    "\n",
    "def data_augment(data, label):\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement \n",
    "    # in the next function (below), this happens essentially for free on TPU. \n",
    "    # Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "    data['inp1'] = tf.image.random_flip_left_right(data['inp1'])\n",
    "    data['inp1'] = tf.image.random_flip_up_down(data['inp1'])\n",
    "    data['inp1'] = tf.image.random_hue(data['inp1'], 0.01)\n",
    "    data['inp1'] = tf.image.random_saturation(data['inp1'], 0.7, 1.3)\n",
    "    data['inp1'] = tf.image.random_contrast(data['inp1'], 0.8, 1.2)\n",
    "    data['inp1'] = tf.image.random_brightness(data['inp1'], 0.1)\n",
    "    \n",
    "    return data, label\n",
    "\n",
    "def get_training_dataset(filenames, labeled = True, ordered = False):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.map(transform, num_parallel_calls = AUTO)\n",
    "    # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(filenames, labeled = True, ordered = True):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # using gpu, not enought memory to use cache\n",
    "    # dataset = dataset.cache()\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(filenames, labeled = False, ordered = True):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.map(setup_input2, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset\n",
    "\n",
    "# function to count how many photos we have in\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "# this function parse our images and also get the target variable\n",
    "def read_tfrecord_full(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string), \n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64), \n",
    "        # meta features\n",
    "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    image_name = example['image_name']\n",
    "    target = tf.cast(example['target'], tf.float32)\n",
    "    # meta features\n",
    "    data = {}\n",
    "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
    "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
    "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
    "    return image, image_name, target, data\n",
    "\n",
    "def load_dataset_full(filenames):        \n",
    "    # automatically interleaves reads from multiple files\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    # returns a dataset of (image_name, target)\n",
    "    dataset = dataset.map(read_tfrecord_full, num_parallel_calls = AUTO) \n",
    "    return dataset\n",
    "\n",
    "def get_data_full(filenames):\n",
    "    dataset = load_dataset_full(filenames)\n",
    "    dataset = dataset.map(setup_input3, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "NUM_TRAINING_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.8)\n",
    "# use validation data for training\n",
    "NUM_VALIDATION_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.2)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "\n",
    "print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 1\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "44113920/44107200 [==============================] - 1s 0us/step\n",
      "Epoch 1/40\n",
      "207/207 - 77s - loss: 0.8239 - auc: 0.5849 - binary_accuracy: 0.6741 - val_loss: 0.2084 - val_auc: 0.6567 - val_binary_accuracy: 0.9835 - lr: 3.0000e-04\n",
      "Epoch 2/40\n",
      "207/207 - 60s - loss: 0.3483 - auc: 0.6565 - binary_accuracy: 0.8791 - val_loss: 0.1906 - val_auc: 0.7707 - val_binary_accuracy: 0.9801 - lr: 3.0000e-04\n",
      "Epoch 3/40\n",
      "207/207 - 62s - loss: 0.2774 - auc: 0.7002 - binary_accuracy: 0.9328 - val_loss: 0.1623 - val_auc: 0.8593 - val_binary_accuracy: 0.9785 - lr: 3.0000e-04\n",
      "Epoch 4/40\n",
      "207/207 - 63s - loss: 0.2372 - auc: 0.7424 - binary_accuracy: 0.9528 - val_loss: 0.1564 - val_auc: 0.8747 - val_binary_accuracy: 0.9803 - lr: 3.0000e-04\n",
      "Epoch 5/40\n",
      "207/207 - 61s - loss: 0.2131 - auc: 0.8082 - binary_accuracy: 0.9585 - val_loss: 0.1633 - val_auc: 0.8703 - val_binary_accuracy: 0.9810 - lr: 3.0000e-04\n",
      "Epoch 6/40\n",
      "207/207 - 63s - loss: 0.1957 - auc: 0.8233 - binary_accuracy: 0.9621 - val_loss: 0.1533 - val_auc: 0.8890 - val_binary_accuracy: 0.9725 - lr: 3.0000e-04\n",
      "Epoch 7/40\n",
      "207/207 - 63s - loss: 0.1891 - auc: 0.8415 - binary_accuracy: 0.9645 - val_loss: 0.1490 - val_auc: 0.8914 - val_binary_accuracy: 0.9789 - lr: 3.0000e-04\n",
      "Epoch 8/40\n",
      "207/207 - 61s - loss: 0.1794 - auc: 0.8633 - binary_accuracy: 0.9627 - val_loss: 0.1587 - val_auc: 0.8702 - val_binary_accuracy: 0.9806 - lr: 3.0000e-04\n",
      "Epoch 9/40\n",
      "207/207 - 63s - loss: 0.1788 - auc: 0.8575 - binary_accuracy: 0.9677 - val_loss: 0.1432 - val_auc: 0.8967 - val_binary_accuracy: 0.9813 - lr: 3.0000e-04\n",
      "Epoch 10/40\n",
      "207/207 - 63s - loss: 0.1687 - auc: 0.8832 - binary_accuracy: 0.9657 - val_loss: 0.1475 - val_auc: 0.8987 - val_binary_accuracy: 0.9732 - lr: 3.0000e-04\n",
      "Epoch 11/40\n",
      "207/207 - 62s - loss: 0.1741 - auc: 0.8663 - binary_accuracy: 0.9685 - val_loss: 0.1694 - val_auc: 0.8729 - val_binary_accuracy: 0.9567 - lr: 3.0000e-04\n",
      "Epoch 12/40\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
      "207/207 - 62s - loss: 0.1542 - auc: 0.8930 - binary_accuracy: 0.9704 - val_loss: 0.1568 - val_auc: 0.8867 - val_binary_accuracy: 0.9819 - lr: 3.0000e-04\n",
      "Epoch 13/40\n",
      "207/207 - 63s - loss: 0.1450 - auc: 0.9113 - binary_accuracy: 0.9719 - val_loss: 0.1506 - val_auc: 0.8941 - val_binary_accuracy: 0.9782 - lr: 1.2000e-04\n",
      "Epoch 14/40\n",
      "207/207 - 65s - loss: 0.1381 - auc: 0.9226 - binary_accuracy: 0.9710 - val_loss: 0.1438 - val_auc: 0.9021 - val_binary_accuracy: 0.9785 - lr: 1.2000e-04\n",
      "Epoch 15/40\n",
      "207/207 - 57s - loss: 0.1348 - auc: 0.9265 - binary_accuracy: 0.9718 - val_loss: 0.1509 - val_auc: 0.9031 - val_binary_accuracy: 0.9784 - lr: 1.2000e-04\n",
      "Epoch 16/40\n",
      "207/207 - 58s - loss: 0.1299 - auc: 0.9348 - binary_accuracy: 0.9713 - val_loss: 0.1495 - val_auc: 0.9110 - val_binary_accuracy: 0.9760 - lr: 1.2000e-04\n",
      "Epoch 17/40\n",
      "207/207 - 58s - loss: 0.1227 - auc: 0.9431 - binary_accuracy: 0.9724 - val_loss: 0.1425 - val_auc: 0.9135 - val_binary_accuracy: 0.9722 - lr: 1.2000e-04\n",
      "Epoch 18/40\n",
      "207/207 - 59s - loss: 0.1162 - auc: 0.9511 - binary_accuracy: 0.9722 - val_loss: 0.1505 - val_auc: 0.9220 - val_binary_accuracy: 0.9549 - lr: 1.2000e-04\n",
      "Epoch 19/40\n",
      "207/207 - 57s - loss: 0.1118 - auc: 0.9555 - binary_accuracy: 0.9722 - val_loss: 0.1643 - val_auc: 0.9030 - val_binary_accuracy: 0.9772 - lr: 1.2000e-04\n",
      "Epoch 20/40\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n",
      "207/207 - 57s - loss: 0.1086 - auc: 0.9582 - binary_accuracy: 0.9735 - val_loss: 0.1704 - val_auc: 0.9000 - val_binary_accuracy: 0.9716 - lr: 1.2000e-04\n",
      "Epoch 21/40\n",
      "207/207 - 59s - loss: 0.0948 - auc: 0.9702 - binary_accuracy: 0.9761 - val_loss: 0.1587 - val_auc: 0.9078 - val_binary_accuracy: 0.9755 - lr: 4.8000e-05\n",
      "Epoch 22/40\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n",
      "207/207 - 59s - loss: 0.0936 - auc: 0.9708 - binary_accuracy: 0.9774 - val_loss: 0.1574 - val_auc: 0.9061 - val_binary_accuracy: 0.9673 - lr: 4.8000e-05\n",
      "Epoch 23/40\n",
      "Restoring model weights from the end of the best epoch.\n",
      "207/207 - 63s - loss: 0.0840 - auc: 0.9772 - binary_accuracy: 0.9806 - val_loss: 0.1577 - val_auc: 0.9077 - val_binary_accuracy: 0.9691 - lr: 1.9200e-05\n",
      "Epoch 00023: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 2\n",
      "Epoch 1/40\n",
      "207/207 - 82s - loss: 0.8222 - auc: 0.5719 - binary_accuracy: 0.6686 - val_loss: 0.3510 - val_auc: 0.6838 - val_binary_accuracy: 0.9134 - lr: 3.0000e-04\n",
      "Epoch 2/40\n",
      "207/207 - 58s - loss: 0.3494 - auc: 0.6417 - binary_accuracy: 0.8780 - val_loss: 0.2040 - val_auc: 0.7900 - val_binary_accuracy: 0.9775 - lr: 3.0000e-04\n",
      "Epoch 3/40\n",
      "207/207 - 59s - loss: 0.2658 - auc: 0.7075 - binary_accuracy: 0.9338 - val_loss: 0.1849 - val_auc: 0.8658 - val_binary_accuracy: 0.9525 - lr: 3.0000e-04\n",
      "Epoch 4/40\n",
      "207/207 - 59s - loss: 0.2270 - auc: 0.7551 - binary_accuracy: 0.9537 - val_loss: 0.1720 - val_auc: 0.8810 - val_binary_accuracy: 0.9802 - lr: 3.0000e-04\n",
      "Epoch 5/40\n",
      "207/207 - 58s - loss: 0.2050 - auc: 0.8093 - binary_accuracy: 0.9590 - val_loss: 0.1875 - val_auc: 0.8964 - val_binary_accuracy: 0.9467 - lr: 3.0000e-04\n",
      "Epoch 6/40\n",
      "207/207 - 57s - loss: 0.1829 - auc: 0.8338 - binary_accuracy: 0.9672 - val_loss: 0.1876 - val_auc: 0.8370 - val_binary_accuracy: 0.9794 - lr: 3.0000e-04\n",
      "Epoch 7/40\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
      "207/207 - 57s - loss: 0.1895 - auc: 0.8315 - binary_accuracy: 0.9658 - val_loss: 0.1921 - val_auc: 0.8505 - val_binary_accuracy: 0.9746 - lr: 3.0000e-04\n",
      "Epoch 8/40\n",
      "207/207 - 59s - loss: 0.1790 - auc: 0.8580 - binary_accuracy: 0.9688 - val_loss: 0.1709 - val_auc: 0.8883 - val_binary_accuracy: 0.9709 - lr: 1.2000e-04\n",
      "Epoch 9/40\n",
      "207/207 - 60s - loss: 0.1595 - auc: 0.8845 - binary_accuracy: 0.9687 - val_loss: 0.1642 - val_auc: 0.9003 - val_binary_accuracy: 0.9771 - lr: 1.2000e-04\n",
      "Epoch 10/40\n",
      "207/207 - 60s - loss: 0.1609 - auc: 0.8781 - binary_accuracy: 0.9707 - val_loss: 0.1581 - val_auc: 0.9092 - val_binary_accuracy: 0.9720 - lr: 1.2000e-04\n",
      "Epoch 11/40\n",
      "207/207 - 59s - loss: 0.1463 - auc: 0.9083 - binary_accuracy: 0.9697 - val_loss: 0.1491 - val_auc: 0.9180 - val_binary_accuracy: 0.9641 - lr: 1.2000e-04\n",
      "Epoch 12/40\n",
      "207/207 - 60s - loss: 0.1429 - auc: 0.9125 - binary_accuracy: 0.9716 - val_loss: 0.1584 - val_auc: 0.9203 - val_binary_accuracy: 0.9472 - lr: 1.2000e-04\n",
      "Epoch 13/40\n",
      "207/207 - 58s - loss: 0.1328 - auc: 0.9240 - binary_accuracy: 0.9719 - val_loss: 0.1499 - val_auc: 0.9176 - val_binary_accuracy: 0.9668 - lr: 1.2000e-04\n",
      "Epoch 14/40\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n",
      "207/207 - 58s - loss: 0.1319 - auc: 0.9247 - binary_accuracy: 0.9724 - val_loss: 0.1595 - val_auc: 0.9038 - val_binary_accuracy: 0.9638 - lr: 1.2000e-04\n",
      "Epoch 15/40\n",
      "207/207 - 58s - loss: 0.1213 - auc: 0.9394 - binary_accuracy: 0.9738 - val_loss: 0.1497 - val_auc: 0.9188 - val_binary_accuracy: 0.9702 - lr: 4.8000e-05\n",
      "Epoch 16/40\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n",
      "207/207 - 58s - loss: 0.1157 - auc: 0.9467 - binary_accuracy: 0.9735 - val_loss: 0.1671 - val_auc: 0.9122 - val_binary_accuracy: 0.9734 - lr: 4.8000e-05\n",
      "Epoch 17/40\n",
      "Restoring model weights from the end of the best epoch.\n",
      "207/207 - 61s - loss: 0.1124 - auc: 0.9485 - binary_accuracy: 0.9752 - val_loss: 0.1677 - val_auc: 0.9081 - val_binary_accuracy: 0.9747 - lr: 1.9200e-05\n",
      "Epoch 00017: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 3\n",
      "Epoch 1/40\n",
      "207/207 - 82s - loss: 0.7954 - auc: 0.5458 - binary_accuracy: 0.6824 - val_loss: 0.2581 - val_auc: 0.6321 - val_binary_accuracy: 0.9775 - lr: 3.0000e-04\n",
      "Epoch 2/40\n",
      "207/207 - 58s - loss: 0.3230 - auc: 0.6469 - binary_accuracy: 0.9009 - val_loss: 0.2255 - val_auc: 0.7685 - val_binary_accuracy: 0.9779 - lr: 3.0000e-04\n",
      "Epoch 3/40\n",
      "207/207 - 58s - loss: 0.2571 - auc: 0.7010 - binary_accuracy: 0.9470 - val_loss: 0.1742 - val_auc: 0.8605 - val_binary_accuracy: 0.9791 - lr: 3.0000e-04\n",
      "Epoch 4/40\n",
      "207/207 - 57s - loss: 0.2155 - auc: 0.7788 - binary_accuracy: 0.9596 - val_loss: 0.1790 - val_auc: 0.8535 - val_binary_accuracy: 0.9800 - lr: 3.0000e-04\n",
      "Epoch 5/40\n",
      "207/207 - 59s - loss: 0.2050 - auc: 0.7936 - binary_accuracy: 0.9642 - val_loss: 0.1677 - val_auc: 0.8730 - val_binary_accuracy: 0.9747 - lr: 3.0000e-04\n",
      "Epoch 6/40\n",
      "207/207 - 59s - loss: 0.1929 - auc: 0.8289 - binary_accuracy: 0.9675 - val_loss: 0.1765 - val_auc: 0.8784 - val_binary_accuracy: 0.9796 - lr: 3.0000e-04\n",
      "Epoch 7/40\n",
      "207/207 - 59s - loss: 0.1804 - auc: 0.8507 - binary_accuracy: 0.9668 - val_loss: 0.1665 - val_auc: 0.8802 - val_binary_accuracy: 0.9688 - lr: 3.0000e-04\n",
      "Epoch 8/40\n",
      "207/207 - 57s - loss: 0.1868 - auc: 0.8311 - binary_accuracy: 0.9688 - val_loss: 0.1837 - val_auc: 0.8608 - val_binary_accuracy: 0.9506 - lr: 3.0000e-04\n",
      "Epoch 9/40\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
      "207/207 - 58s - loss: 0.1886 - auc: 0.8385 - binary_accuracy: 0.9678 - val_loss: 0.1888 - val_auc: 0.8542 - val_binary_accuracy: 0.9524 - lr: 3.0000e-04\n",
      "Epoch 10/40\n",
      "207/207 - 58s - loss: 0.1915 - auc: 0.8305 - binary_accuracy: 0.9683 - val_loss: 0.1649 - val_auc: 0.8783 - val_binary_accuracy: 0.9751 - lr: 1.2000e-04\n",
      "Epoch 11/40\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n",
      "207/207 - 59s - loss: 0.1637 - auc: 0.8739 - binary_accuracy: 0.9702 - val_loss: 0.1709 - val_auc: 0.8763 - val_binary_accuracy: 0.9718 - lr: 1.2000e-04\n",
      "Epoch 12/40\n",
      "207/207 - 61s - loss: 0.1713 - auc: 0.8689 - binary_accuracy: 0.9691 - val_loss: 0.1635 - val_auc: 0.8868 - val_binary_accuracy: 0.9767 - lr: 4.8000e-05\n",
      "Epoch 13/40\n",
      "207/207 - 60s - loss: 0.1585 - auc: 0.8870 - binary_accuracy: 0.9719 - val_loss: 0.1609 - val_auc: 0.8896 - val_binary_accuracy: 0.9742 - lr: 4.8000e-05\n",
      "Epoch 14/40\n",
      "207/207 - 60s - loss: 0.1592 - auc: 0.8834 - binary_accuracy: 0.9716 - val_loss: 0.1594 - val_auc: 0.8951 - val_binary_accuracy: 0.9744 - lr: 4.8000e-05\n",
      "Epoch 15/40\n",
      "207/207 - 58s - loss: 0.1490 - auc: 0.9009 - binary_accuracy: 0.9716 - val_loss: 0.1628 - val_auc: 0.8897 - val_binary_accuracy: 0.9723 - lr: 4.8000e-05\n",
      "Epoch 16/40\n",
      "207/207 - 60s - loss: 0.1474 - auc: 0.9025 - binary_accuracy: 0.9716 - val_loss: 0.1606 - val_auc: 0.8957 - val_binary_accuracy: 0.9763 - lr: 4.8000e-05\n",
      "Epoch 17/40\n",
      "207/207 - 60s - loss: 0.1533 - auc: 0.8976 - binary_accuracy: 0.9728 - val_loss: 0.1640 - val_auc: 0.8969 - val_binary_accuracy: 0.9768 - lr: 4.8000e-05\n",
      "Epoch 18/40\n",
      "207/207 - 58s - loss: 0.1460 - auc: 0.9051 - binary_accuracy: 0.9723 - val_loss: 0.1668 - val_auc: 0.8944 - val_binary_accuracy: 0.9733 - lr: 4.8000e-05\n",
      "Epoch 19/40\n",
      "207/207 - 60s - loss: 0.1501 - auc: 0.9025 - binary_accuracy: 0.9697 - val_loss: 0.1558 - val_auc: 0.8984 - val_binary_accuracy: 0.9744 - lr: 4.8000e-05\n",
      "Epoch 20/40\n",
      "207/207 - 60s - loss: 0.1402 - auc: 0.9148 - binary_accuracy: 0.9726 - val_loss: 0.1601 - val_auc: 0.9034 - val_binary_accuracy: 0.9773 - lr: 4.8000e-05\n",
      "Epoch 21/40\n",
      "207/207 - 60s - loss: 0.1452 - auc: 0.9078 - binary_accuracy: 0.9721 - val_loss: 0.1571 - val_auc: 0.9047 - val_binary_accuracy: 0.9738 - lr: 4.8000e-05\n",
      "Epoch 22/40\n",
      "207/207 - 60s - loss: 0.1385 - auc: 0.9198 - binary_accuracy: 0.9711 - val_loss: 0.1591 - val_auc: 0.9062 - val_binary_accuracy: 0.9672 - lr: 4.8000e-05\n",
      "Epoch 23/40\n",
      "207/207 - 59s - loss: 0.1329 - auc: 0.9246 - binary_accuracy: 0.9720 - val_loss: 0.1551 - val_auc: 0.9090 - val_binary_accuracy: 0.9680 - lr: 4.8000e-05\n",
      "Epoch 24/40\n",
      "207/207 - 59s - loss: 0.1299 - auc: 0.9325 - binary_accuracy: 0.9725 - val_loss: 0.1582 - val_auc: 0.9129 - val_binary_accuracy: 0.9697 - lr: 4.8000e-05\n",
      "Epoch 25/40\n",
      "207/207 - 59s - loss: 0.1319 - auc: 0.9309 - binary_accuracy: 0.9730 - val_loss: 0.1569 - val_auc: 0.9134 - val_binary_accuracy: 0.9702 - lr: 4.8000e-05\n",
      "Epoch 26/40\n",
      "207/207 - 59s - loss: 0.1261 - auc: 0.9337 - binary_accuracy: 0.9722 - val_loss: 0.1597 - val_auc: 0.9214 - val_binary_accuracy: 0.9739 - lr: 4.8000e-05\n",
      "Epoch 27/40\n",
      "207/207 - 59s - loss: 0.1251 - auc: 0.9369 - binary_accuracy: 0.9746 - val_loss: 0.1503 - val_auc: 0.9243 - val_binary_accuracy: 0.9664 - lr: 4.8000e-05\n",
      "Epoch 28/40\n",
      "207/207 - 57s - loss: 0.1328 - auc: 0.9305 - binary_accuracy: 0.9748 - val_loss: 0.1517 - val_auc: 0.9233 - val_binary_accuracy: 0.9739 - lr: 4.8000e-05\n",
      "Epoch 29/40\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n",
      "207/207 - 57s - loss: 0.1183 - auc: 0.9429 - binary_accuracy: 0.9751 - val_loss: 0.1551 - val_auc: 0.9142 - val_binary_accuracy: 0.9730 - lr: 4.8000e-05\n",
      "Epoch 30/40\n",
      "207/207 - 58s - loss: 0.1175 - auc: 0.9452 - binary_accuracy: 0.9758 - val_loss: 0.1530 - val_auc: 0.9173 - val_binary_accuracy: 0.9707 - lr: 1.9200e-05\n",
      "Epoch 31/40\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 7.680000271648168e-06.\n",
      "207/207 - 58s - loss: 0.1098 - auc: 0.9547 - binary_accuracy: 0.9763 - val_loss: 0.1555 - val_auc: 0.9192 - val_binary_accuracy: 0.9675 - lr: 1.9200e-05\n",
      "Epoch 32/40\n",
      "Restoring model weights from the end of the best epoch.\n",
      "207/207 - 61s - loss: 0.1090 - auc: 0.9532 - binary_accuracy: 0.9768 - val_loss: 0.1553 - val_auc: 0.9181 - val_binary_accuracy: 0.9680 - lr: 7.6800e-06\n",
      "Epoch 00032: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 4\n",
      "Epoch 1/40\n",
      "207/207 - 75s - loss: 0.8080 - auc: 0.5744 - binary_accuracy: 0.6716 - val_loss: 0.2110 - val_auc: 0.5922 - val_binary_accuracy: 0.9842 - lr: 3.0000e-04\n",
      "Epoch 2/40\n",
      "207/207 - 59s - loss: 0.3520 - auc: 0.6435 - binary_accuracy: 0.8768 - val_loss: 0.1971 - val_auc: 0.7193 - val_binary_accuracy: 0.9818 - lr: 3.0000e-04\n",
      "Epoch 3/40\n",
      "207/207 - 60s - loss: 0.2619 - auc: 0.7171 - binary_accuracy: 0.9346 - val_loss: 0.1671 - val_auc: 0.8349 - val_binary_accuracy: 0.9750 - lr: 3.0000e-04\n",
      "Epoch 4/40\n",
      "207/207 - 60s - loss: 0.2281 - auc: 0.7641 - binary_accuracy: 0.9529 - val_loss: 0.1615 - val_auc: 0.8562 - val_binary_accuracy: 0.9719 - lr: 3.0000e-04\n",
      "Epoch 5/40\n",
      "207/207 - 59s - loss: 0.2051 - auc: 0.8033 - binary_accuracy: 0.9611 - val_loss: 0.1581 - val_auc: 0.8577 - val_binary_accuracy: 0.9813 - lr: 3.0000e-04\n",
      "Epoch 6/40\n",
      "207/207 - 59s - loss: 0.1996 - auc: 0.8180 - binary_accuracy: 0.9618 - val_loss: 0.1520 - val_auc: 0.8768 - val_binary_accuracy: 0.9832 - lr: 3.0000e-04\n",
      "Epoch 7/40\n",
      "207/207 - 58s - loss: 0.1756 - auc: 0.8621 - binary_accuracy: 0.9678 - val_loss: 0.1609 - val_auc: 0.8632 - val_binary_accuracy: 0.9769 - lr: 3.0000e-04\n",
      "Epoch 8/40\n",
      "207/207 - 59s - loss: 0.1723 - auc: 0.8656 - binary_accuracy: 0.9681 - val_loss: 0.1442 - val_auc: 0.8921 - val_binary_accuracy: 0.9829 - lr: 3.0000e-04\n",
      "Epoch 9/40\n",
      "207/207 - 57s - loss: 0.1677 - auc: 0.8753 - binary_accuracy: 0.9697 - val_loss: 0.1492 - val_auc: 0.8878 - val_binary_accuracy: 0.9700 - lr: 3.0000e-04\n",
      "Epoch 10/40\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
      "207/207 - 58s - loss: 0.1669 - auc: 0.8790 - binary_accuracy: 0.9708 - val_loss: 0.1634 - val_auc: 0.8727 - val_binary_accuracy: 0.9773 - lr: 3.0000e-04\n",
      "Epoch 11/40\n",
      "207/207 - 58s - loss: 0.1742 - auc: 0.8768 - binary_accuracy: 0.9689 - val_loss: 0.1485 - val_auc: 0.8919 - val_binary_accuracy: 0.9757 - lr: 1.2000e-04\n",
      "Epoch 12/40\n",
      "207/207 - 60s - loss: 0.1479 - auc: 0.9055 - binary_accuracy: 0.9724 - val_loss: 0.1428 - val_auc: 0.9019 - val_binary_accuracy: 0.9776 - lr: 1.2000e-04\n",
      "Epoch 13/40\n",
      "207/207 - 58s - loss: 0.1428 - auc: 0.9184 - binary_accuracy: 0.9699 - val_loss: 0.1451 - val_auc: 0.8995 - val_binary_accuracy: 0.9731 - lr: 1.2000e-04\n",
      "Epoch 14/40\n",
      "207/207 - 60s - loss: 0.1299 - auc: 0.9311 - binary_accuracy: 0.9725 - val_loss: 0.1436 - val_auc: 0.9046 - val_binary_accuracy: 0.9705 - lr: 1.2000e-04\n",
      "Epoch 15/40\n",
      "207/207 - 60s - loss: 0.1277 - auc: 0.9378 - binary_accuracy: 0.9732 - val_loss: 0.1412 - val_auc: 0.9085 - val_binary_accuracy: 0.9718 - lr: 1.2000e-04\n",
      "Epoch 16/40\n",
      "207/207 - 58s - loss: 0.1321 - auc: 0.9302 - binary_accuracy: 0.9721 - val_loss: 0.1605 - val_auc: 0.8960 - val_binary_accuracy: 0.9818 - lr: 1.2000e-04\n",
      "Epoch 17/40\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n",
      "207/207 - 58s - loss: 0.1220 - auc: 0.9447 - binary_accuracy: 0.9731 - val_loss: 0.1581 - val_auc: 0.8922 - val_binary_accuracy: 0.9771 - lr: 1.2000e-04\n",
      "Epoch 18/40\n",
      "207/207 - 58s - loss: 0.1178 - auc: 0.9482 - binary_accuracy: 0.9740 - val_loss: 0.1482 - val_auc: 0.9049 - val_binary_accuracy: 0.9710 - lr: 4.8000e-05\n",
      "Epoch 19/40\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n",
      "207/207 - 58s - loss: 0.1085 - auc: 0.9569 - binary_accuracy: 0.9760 - val_loss: 0.1523 - val_auc: 0.9004 - val_binary_accuracy: 0.9732 - lr: 4.8000e-05\n",
      "Epoch 20/40\n",
      "Restoring model weights from the end of the best epoch.\n",
      "207/207 - 61s - loss: 0.1028 - auc: 0.9594 - binary_accuracy: 0.9778 - val_loss: 0.1521 - val_auc: 0.8983 - val_binary_accuracy: 0.9692 - lr: 1.9200e-05\n",
      "Epoch 00020: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 5\n",
      "Epoch 1/40\n",
      "207/207 - 81s - loss: 0.8139 - auc: 0.5715 - binary_accuracy: 0.6686 - val_loss: 0.2385 - val_auc: 0.5971 - val_binary_accuracy: 0.9812 - lr: 3.0000e-04\n",
      "Epoch 2/40\n",
      "207/207 - 58s - loss: 0.3544 - auc: 0.6313 - binary_accuracy: 0.8744 - val_loss: 0.1867 - val_auc: 0.8168 - val_binary_accuracy: 0.9802 - lr: 3.0000e-04\n",
      "Epoch 3/40\n",
      "207/207 - 59s - loss: 0.2570 - auc: 0.6995 - binary_accuracy: 0.9411 - val_loss: 0.1796 - val_auc: 0.8521 - val_binary_accuracy: 0.9794 - lr: 3.0000e-04\n",
      "Epoch 4/40\n",
      "207/207 - 59s - loss: 0.2214 - auc: 0.7698 - binary_accuracy: 0.9585 - val_loss: 0.1717 - val_auc: 0.8633 - val_binary_accuracy: 0.9791 - lr: 3.0000e-04\n",
      "Epoch 5/40\n",
      "207/207 - 59s - loss: 0.1999 - auc: 0.7937 - binary_accuracy: 0.9650 - val_loss: 0.1645 - val_auc: 0.8786 - val_binary_accuracy: 0.9688 - lr: 3.0000e-04\n",
      "Epoch 6/40\n",
      "207/207 - 57s - loss: 0.1883 - auc: 0.8229 - binary_accuracy: 0.9664 - val_loss: 0.1828 - val_auc: 0.8635 - val_binary_accuracy: 0.9813 - lr: 3.0000e-04\n",
      "Epoch 7/40\n",
      "207/207 - 59s - loss: 0.1772 - auc: 0.8487 - binary_accuracy: 0.9701 - val_loss: 0.1618 - val_auc: 0.8843 - val_binary_accuracy: 0.9771 - lr: 3.0000e-04\n",
      "Epoch 8/40\n",
      "207/207 - 59s - loss: 0.1788 - auc: 0.8457 - binary_accuracy: 0.9703 - val_loss: 0.1510 - val_auc: 0.9010 - val_binary_accuracy: 0.9805 - lr: 3.0000e-04\n",
      "Epoch 9/40\n",
      "207/207 - 57s - loss: 0.1770 - auc: 0.8569 - binary_accuracy: 0.9698 - val_loss: 0.1595 - val_auc: 0.8893 - val_binary_accuracy: 0.9683 - lr: 3.0000e-04\n",
      "Epoch 10/40\n",
      "207/207 - 60s - loss: 0.1693 - auc: 0.8769 - binary_accuracy: 0.9698 - val_loss: 0.1474 - val_auc: 0.9120 - val_binary_accuracy: 0.9807 - lr: 3.0000e-04\n",
      "Epoch 11/40\n",
      "207/207 - 58s - loss: 0.1634 - auc: 0.8712 - binary_accuracy: 0.9735 - val_loss: 0.1495 - val_auc: 0.9052 - val_binary_accuracy: 0.9720 - lr: 3.0000e-04\n",
      "Epoch 12/40\n",
      "207/207 - 59s - loss: 0.1583 - auc: 0.8822 - binary_accuracy: 0.9722 - val_loss: 0.1474 - val_auc: 0.9179 - val_binary_accuracy: 0.9800 - lr: 3.0000e-04\n",
      "Epoch 13/40\n",
      "207/207 - 58s - loss: 0.1515 - auc: 0.9004 - binary_accuracy: 0.9703 - val_loss: 0.1728 - val_auc: 0.9041 - val_binary_accuracy: 0.9792 - lr: 3.0000e-04\n",
      "Epoch 14/40\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
      "207/207 - 58s - loss: 0.1565 - auc: 0.8865 - binary_accuracy: 0.9720 - val_loss: 0.1593 - val_auc: 0.8972 - val_binary_accuracy: 0.9763 - lr: 3.0000e-04\n",
      "Epoch 15/40\n",
      "207/207 - 58s - loss: 0.1472 - auc: 0.9043 - binary_accuracy: 0.9744 - val_loss: 0.1608 - val_auc: 0.9120 - val_binary_accuracy: 0.9744 - lr: 1.2000e-04\n",
      "Epoch 16/40\n",
      "207/207 - 60s - loss: 0.1312 - auc: 0.9297 - binary_accuracy: 0.9732 - val_loss: 0.1438 - val_auc: 0.9229 - val_binary_accuracy: 0.9701 - lr: 1.2000e-04\n",
      "Epoch 17/40\n",
      "207/207 - 58s - loss: 0.1341 - auc: 0.9250 - binary_accuracy: 0.9725 - val_loss: 0.1476 - val_auc: 0.9201 - val_binary_accuracy: 0.9730 - lr: 1.2000e-04\n",
      "Epoch 18/40\n",
      "207/207 - 60s - loss: 0.1291 - auc: 0.9338 - binary_accuracy: 0.9729 - val_loss: 0.1400 - val_auc: 0.9311 - val_binary_accuracy: 0.9717 - lr: 1.2000e-04\n",
      "Epoch 19/40\n",
      "207/207 - 58s - loss: 0.1271 - auc: 0.9306 - binary_accuracy: 0.9729 - val_loss: 0.1476 - val_auc: 0.9189 - val_binary_accuracy: 0.9714 - lr: 1.2000e-04\n",
      "Epoch 20/40\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n",
      "207/207 - 58s - loss: 0.1262 - auc: 0.9360 - binary_accuracy: 0.9731 - val_loss: 0.1543 - val_auc: 0.9223 - val_binary_accuracy: 0.9796 - lr: 1.2000e-04\n",
      "Epoch 21/40\n",
      "207/207 - 58s - loss: 0.1126 - auc: 0.9496 - binary_accuracy: 0.9754 - val_loss: 0.1367 - val_auc: 0.9292 - val_binary_accuracy: 0.9702 - lr: 4.8000e-05\n",
      "Epoch 22/40\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n",
      "207/207 - 58s - loss: 0.1061 - auc: 0.9569 - binary_accuracy: 0.9769 - val_loss: 0.1397 - val_auc: 0.9275 - val_binary_accuracy: 0.9688 - lr: 4.8000e-05\n",
      "Epoch 23/40\n",
      "Restoring model weights from the end of the best epoch.\n",
      "207/207 - 61s - loss: 0.1048 - auc: 0.9582 - binary_accuracy: 0.9768 - val_loss: 0.1433 - val_auc: 0.9257 - val_binary_accuracy: 0.9739 - lr: 1.9200e-05\n",
      "Epoch 00023: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Computing predictions...\n",
      "Generating submission.csv file...\n"
     ]
    }
   ],
   "source": [
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    \n",
    "    with strategy.scope():\n",
    "        inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "        inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n",
    "        efnetb3 = efn.EfficientNetB3(weights = 'imagenet', include_top = False)\n",
    "        x = efnetb3(inp1)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x1 = tf.keras.layers.Dense(50)(inp2)\n",
    "        x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "        x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "        concat = tf.keras.layers.concatenate([x, x1])\n",
    "        concat = tf.keras.layers.Dense(512, activation = 'relu')(concat)\n",
    "        concat = tf.keras.layers.BatchNormalization()(concat)\n",
    "        concat = tf.keras.layers.Dropout(0.2)(concat)\n",
    "        concat = tf.keras.layers.Dense(182, activation = 'relu')(concat)\n",
    "        concat = tf.keras.layers.BatchNormalization()(concat)\n",
    "        concat = tf.keras.layers.Dropout(0.2)(concat)\n",
    "        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "        # opt = tfa.optimizers.SWA(opt)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer = opt,\n",
    "            loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n",
    "            metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n",
    "        )\n",
    "\n",
    "        return model\n",
    "    \n",
    "def train_and_predict(SUB, folds = 5):\n",
    "    \n",
    "    models = []\n",
    "    oof_image_name = []\n",
    "    oof_target = []\n",
    "    oof_prediction = []\n",
    "    \n",
    "    # seed everything\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    kfold = KFold(folds, shuffle = True, random_state = SEED)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        train_dataset = get_training_dataset([TRAINING_FILENAMES[x] for x in trn_ind], labeled = True, ordered = False)\n",
    "        val_dataset = get_validation_dataset([TRAINING_FILENAMES[x] for x in val_ind], labeled = True, ordered = True)\n",
    "        K.clear_session()\n",
    "        model = get_model()\n",
    "        # using early stopping using val loss\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_auc', mode = 'max', patience = 5, \n",
    "                                                      verbose = 1, min_delta = 0.0001, restore_best_weights = True)\n",
    "        # lr scheduler\n",
    "        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_auc', factor = 0.4, patience = 2, verbose = 1, min_delta = 0.0001, mode = 'max')\n",
    "        history = model.fit(train_dataset, \n",
    "                            steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                            epochs = EPOCHS,\n",
    "                            callbacks = [early_stopping, cb_lr_schedule],\n",
    "                            validation_data = val_dataset,\n",
    "                            verbose = 2)\n",
    "        models.append(model)\n",
    "        \n",
    "        # want to predict the validation set and save them for stacking\n",
    "        number_of_files = count_data_items([TRAINING_FILENAMES[x] for x in val_ind])\n",
    "        dataset = get_data_full([TRAINING_FILENAMES[x] for x in val_ind])\n",
    "        # get the image name\n",
    "        image_name = dataset.map(lambda image, image_name, target: image_name).unbatch()\n",
    "        image_name = next(iter(image_name.batch(number_of_files))).numpy().astype('U')\n",
    "        # get the real target\n",
    "        target = dataset.map(lambda image, image_name, target: target).unbatch()\n",
    "        target = next(iter(target.batch(number_of_files))).numpy()\n",
    "        # predict the validation set\n",
    "        image = dataset.map(lambda image, image_name, target: image)\n",
    "        probabilities = model.predict(image)\n",
    "        oof_image_name.extend(list(image_name))\n",
    "        oof_target.extend(list(target))\n",
    "        oof_prediction.extend(list(np.concatenate(probabilities)))\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    # save oof predictions\n",
    "    oof_df = pd.DataFrame({'image_name': oof_image_name, 'target': oof_target, 'predictions': oof_prediction})\n",
    "    oof_df.to_csv('EfficientNetB3_384.csv', index = False)\n",
    "        \n",
    "    # since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
    "    test_ds = get_test_dataset(TEST_FILENAMES, labeled = False, ordered = True)\n",
    "    test_images_ds = test_ds.map(lambda image, image_name: image)\n",
    "    \n",
    "    print('Computing predictions...')\n",
    "    probabilities = np.average([np.concatenate(models[i].predict(test_images_ds)) for i in range(folds)], axis = 0)\n",
    "    print('Generating submission.csv file...')\n",
    "    test_ids_ds = test_ds.map(lambda image, image_name: image_name).unbatch()\n",
    "    # all in one batch\n",
    "    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "    pred_df = pd.DataFrame({'image_name': test_ids, 'target': probabilities})\n",
    "    SUB.drop('target', inplace = True, axis = 1)\n",
    "    SUB = SUB.merge(pred_df, on = 'image_name')\n",
    "    SUB.to_csv('sub_EfficientNetB3_384.csv', index = False)\n",
    "    \n",
    "    return oof_target, oof_prediction\n",
    "    \n",
    "oof_target, oof_prediction = train_and_predict(SUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our out of folds roc auc score is:  0.9194544584322221\n"
     ]
    }
   ],
   "source": [
    "# calculate our out of folds roc auc score\n",
    "roc_auc = metrics.roc_auc_score(oof_target, oof_prediction)\n",
    "print('Our out of folds roc auc score is: ', roc_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
